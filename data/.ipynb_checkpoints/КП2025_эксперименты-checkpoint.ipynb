{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Подготовка пайплайна тестирования"
   ],
   "metadata": {
    "id": "YBmI8eNC00TX"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Установка пакета с алгоритмами"
   ],
   "metadata": {
    "id": "cbi-009v0_hn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install cp2025"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kWpshXeu0-6w",
    "outputId": "0bf5b0d3-40f9-4151-e1cc-ac816497cdb0"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting cp2025\n",
      "  Downloading cp2025-1.0.2-py3-none-any.whl.metadata (70 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m70.1/70.1 kB\u001B[0m \u001B[31m1.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting ECPy==1.2.5 (from cp2025)\n",
      "  Downloading ECPy-1.2.5-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting importlib_metadata==7.0.1 (from cp2025)\n",
      "  Downloading importlib_metadata-7.0.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting numpy==1.26.4 (from cp2025)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m61.0/61.0 kB\u001B[0m \u001B[31m4.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.11/dist-packages (from cp2025) (2.2.2)\n",
      "Collecting scikit_learn==1.4.2 (from cp2025)\n",
      "  Downloading scikit_learn-1.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting scipy==1.13.1 (from cp2025)\n",
      "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m60.6/60.6 kB\u001B[0m \u001B[31m3.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata==7.0.1->cp2025) (3.21.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2->cp2025) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2->cp2025) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2->cp2025) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit_learn==1.4.2->cp2025) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit_learn==1.4.2->cp2025) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2->cp2025) (1.17.0)\n",
      "Downloading cp2025-1.0.2-py3-none-any.whl (60 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m60.5/60.5 kB\u001B[0m \u001B[31m4.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading ECPy-1.2.5-py3-none-any.whl (43 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m43.1/43.1 kB\u001B[0m \u001B[31m2.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading importlib_metadata-7.0.1-py3-none-any.whl (23 kB)\n",
      "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m18.3/18.3 MB\u001B[0m \u001B[31m36.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading scikit_learn-1.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.1/12.1 MB\u001B[0m \u001B[31m39.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m38.6/38.6 MB\u001B[0m \u001B[31m13.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: ECPy, numpy, importlib_metadata, scipy, scikit_learn, cp2025\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: importlib_metadata\n",
      "    Found existing installation: importlib_metadata 8.7.0\n",
      "    Uninstalling importlib_metadata-8.7.0:\n",
      "      Successfully uninstalled importlib_metadata-8.7.0\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.15.2\n",
      "    Uninstalling scipy-1.15.2:\n",
      "      Successfully uninstalled scipy-1.15.2\n",
      "  Attempting uninstall: scikit_learn\n",
      "    Found existing installation: scikit-learn 1.6.1\n",
      "    Uninstalling scikit-learn-1.6.1:\n",
      "      Successfully uninstalled scikit-learn-1.6.1\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed ECPy-1.2.5 cp2025-1.0.2 importlib_metadata-7.0.1 numpy-1.26.4 scikit_learn-1.4.2 scipy-1.13.1\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "importlib_metadata"
        ]
       },
       "id": "59084fd949584f0b84dd44975a24b9a1"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Импорт зависимостей"
   ],
   "metadata": {
    "id": "GE34TXHwg9I_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from cp2025.algorithms.AggregationKAnonymityTimeOptimal import AggregationKAnonymityTimeOptimal\n",
    "from cp2025.algorithms.AggregationLDiversityTimeOptimal import AggregationLDiversityTimeOptimal\n",
    "from cp2025.algorithms.Datafly import Datafly\n",
    "from cp2025.algorithms.GeneralizationGreedyByOneEqualSizedGroups import GeneralizationGreedyByOneEqualSizedGroups\n",
    "from cp2025.algorithms.GeneralizationKAnonymityTimeOptimal import GeneralizationKAnonymityTimeOptimal\n",
    "from cp2025.algorithms.GeneralizationLDiversityTimeOptimal import GeneralizationLDiversityTimeOptimal\n",
    "from cp2025.algorithms.groupjoin import *\n",
    "from cp2025.algorithms.IdentifierHasher import IdentifierHasher\n",
    "from cp2025.algorithms.RandomizationDepersonalizator import RandomizationBaselineDepersonalizator\n",
    "from cp2025.algorithms.Shuffler import Shuffler\n",
    "from cp2025.algorithms.ShufflerInBatches import ShufflerInBatches\n",
    "from cp2025.algorithms.SuppressionKAnonymityBaseline import SuppressionKAnonymityBaseline\n",
    "from cp2025.algorithms.SuppressionKAnonymityTimeOptimal import SuppressionKAnonymityTimeOptimal\n",
    "from cp2025.algorithms.SuppressionLDiversityBaseline import SuppressionLDiversityBaseline\n",
    "from cp2025.algorithms.SuppressionLDiversityTimeOptimal import SuppressionLDiversityTimeOptimal\n",
    "from cp2025.algorithms.SuppressionTClosenessBaseline import SuppressionTClosenessBaseline\n",
    "from cp2025.utility.metrics import *\n",
    "from cp2025.utility.boosting_security_score import get_boosting_security_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from cp2025.utility.prepare_data import prepare_data\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ],
   "metadata": {
    "id": "nCwBzrdSg8qb",
    "ExecuteTime": {
     "end_time": "2025-05-08T19:13:01.309292Z",
     "start_time": "2025-05-08T19:13:00.561280Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "Функция оценки качества обезличивания при помощи следующих мертик:\n",
    "\n",
    "Для полезности обезличенных данных:\n",
    "\n",
    "* средний размер класса эквивалентности\n",
    "* отношение количества уникальных записей к общему числу записей (distinctness)\n",
    "* доля изменившихся отдельных значений в датасете\n",
    "* неоднородная энтропия (non-uniform entropy)\n",
    "* потеря информации на основе энтропии (entropy-based information loss)\n",
    "* оценка расстояния между исходными и получившимися данными поэлементно (my_by_element_distance)\n",
    "* решение задач классификации или регрессии при помощи градиентного бустинга на изначальном и обезличенном датасетах и сравнение качества работы моделей\n",
    "\n",
    "\n",
    "Для рисков деобезличивания:\n",
    "\n",
    "* оценка максимального k, при котором выполняется k-анонимность\n",
    "* оценка максимального l, при котором выполняется l-разнообразие\n",
    "* оценка минимального t, при котором выполняется t-близость\n",
    "* adversarial knowledge gain, описанный в пункте 4.4 статьи https://desfontain.es/PDFs/PhD/TheCostOfPrivacyDestructionOfDataMiningUtilityInAnonymizedDataPublishing.pdf\n",
    "* adversarial accuracy gain, описанный в пункте 4.4 статьи https://desfontain.es/PDFs/PhD/TheCostOfPrivacyDestructionOfDataMiningUtilityInAnonymizedDataPublishing.pdf\n",
    "* оценка accuracy деобезличивания при помощи градиентного бустинга\n"
   ],
   "metadata": {
    "id": "i686TNYebV-g"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def score_depersonalization_quality(initial_df, depersonalized_df, quasi_identifiers_columns_ids, quasi_identifiers_columns_types,\n",
    "                                    sensetive_columns_ids, sensetive_columns_types, predicted_column, generalization = False):\n",
    "    global seed\n",
    "\n",
    "    # depersonalization risk\n",
    "    k = find_k_anonimus(depersonalized_df[:,quasi_identifiers_columns_ids])\n",
    "    print(f\"Максимальное k: {k}\")\n",
    "    #l = find_l_diverse(depersonalized_df[:,quasi_identifiers_columns_ids], depersonalized_df[:,sensetive_columns_ids]) # TODO: fix to work with many sensetives\n",
    "    #t = find_t_close(depersonalized_df[:,quasi_identifiers_columns_ids], depersonalized_df[:,sensetive_columns_ids], sensetive_columns_types) # TODO: fix to work with many sensetives\n",
    "    #adversarial_knowledge = adversarial_knowledge_gain(depersonalized_df[:,quasi_identifiers_columns_ids], depersonalized_df[:,sensetive_columns_ids]) # TODO: fix to work with many sensetives\n",
    "    #adversarial_accuracy = adversarial_accuracy_gain(depersonalized_df[:,quasi_identifiers_columns_ids], depersonalized_df[:,sensetive_columns_ids]) # TODO: fix to work with many sensetives\n",
    "\n",
    "    # security boosting\n",
    "    train_initial_before_prepare, test_initial_before_prepare = train_test_split(initial_df[:,quasi_identifiers_columns_ids + sensetive_columns_ids], test_size=0.2, random_state=seed, shuffle=False)\n",
    "    train_depersonalized, test_depersonalized = train_test_split(depersonalized_df[:,quasi_identifiers_columns_ids + sensetive_columns_ids], test_size=0.2, random_state=seed, shuffle=False)\n",
    "    train_initial, test_initial = prepare_data(train_initial_before_prepare, test_initial_before_prepare, [generalization] * len(quasi_identifiers_columns_types) + [0]*len(sensetive_columns_types), quasi_identifiers_columns_types + sensetive_columns_types, normalize=False)\n",
    "    train_depersonalized, test_depersonalized = prepare_data(train_depersonalized, test_depersonalized, [generalization] * len(quasi_identifiers_columns_types) + [0]*len(sensetive_columns_types), quasi_identifiers_columns_types + sensetive_columns_types, normalize=False)\n",
    "    security_boosting_score = get_boosting_security_score(train_initial, train_depersonalized, test_initial, test_depersonalized)\n",
    "    print(f\"security boosting score: {security_boosting_score}\")\n",
    "\n",
    "    # utility\n",
    "    average_equivalence_class = average_equivalence_class_size(depersonalized_df[:,quasi_identifiers_columns_ids])\n",
    "    print(average_equivalence_class)\n",
    "    distinctness_score = distinctness(depersonalized_df[:,quasi_identifiers_columns_ids])\n",
    "    print(distinctness_score)\n",
    "    changed_proportion_score = changed_proportion(initial_df[:,quasi_identifiers_columns_ids], depersonalized_df[:,quasi_identifiers_columns_ids])\n",
    "    print(changed_proportion_score)\n",
    "    non_uniform_entropy_score = non_uniform_entropy(depersonalized_df[:,quasi_identifiers_columns_ids])\n",
    "    print(non_uniform_entropy_score)\n",
    "    entropy_based_information_loss_score = entropy_based_information_loss(initial_df[:,quasi_identifiers_columns_ids], depersonalized_df[:,quasi_identifiers_columns_ids])\n",
    "    print(entropy_based_information_loss_score)\n",
    "    my_by_element_distance_score = my_by_element_distance(initial_df[:,quasi_identifiers_columns_ids], depersonalized_df[:,quasi_identifiers_columns_ids], quasi_identifiers_columns_types)\n",
    "    print(my_by_element_distance_score)\n",
    "\n",
    "    # utility boosting\n",
    "    parameters = {\n",
    "        'n_estimators':list(range(10, 201, 15)),\n",
    "        #'alpha':[i / 10 for i in range(1, 10, 4)],\n",
    "        'min_samples_leaf':[i for i in range(1, 6)],\n",
    "        'criterion':['squared_error'],\n",
    "    }\n",
    "    reg = GridSearchCV(GradientBoostingRegressor(random_state=seed), parameters)\n",
    "    y_train = train_initial_before_prepare[:,predicted_column]\n",
    "    y_test = test_initial_before_prepare[:,predicted_column]\n",
    "    reg.fit(train_depersonalized, y_train)\n",
    "    res = reg.predict(test_depersonalized)\n",
    "    utility_boosting_score = ((res - y_test)**2).mean()**0.5\n",
    "    print(utility_boosting_score)"
   ],
   "metadata": {
    "id": "Uu31tXWNbVu8",
    "ExecuteTime": {
     "end_time": "2025-05-08T19:13:01.324922Z",
     "start_time": "2025-05-08T19:13:01.319298Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "Загрузка датасетов:\n",
    "\n",
    "* df_caggle - датасет с данными по займам с caggle\n",
    "* df_generated - датасет, сгенерированный ChatGpt (идентификаторы убраны)\n",
    "* df_short - укороченный до 30 строк df_caggle для baseline алгоритмов (так как задача достижения оптимальной по потере полезности данных k-анонимности - NP-сложная задача)"
   ],
   "metadata": {
    "id": "xyY7qxy-mASD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#!wget https://raw.githubusercontent.com/ShompolovMaxim/CP_2025/refs/heads/main/data/Bank_Personal_Loan_Modelling.csv\n",
    "#!wget https://raw.githubusercontent.com/ShompolovMaxim/CP_2025/refs/heads/main/data/generated.csv\n",
    "df_caggle = pd.read_csv(\"Bank_Personal_Loan_Modelling.csv\", keep_default_na=False)\n",
    "df_generated = pd.read_csv(\"generated.csv\", keep_default_na=False)\n",
    "df_short = df_caggle[:30]\n",
    "df_caggle = df_caggle[:1000]\n",
    "df_generated = df_generated[:1000]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z0ZYnZvwmD1I",
    "outputId": "42bd465b-7843-426f-fb24-f5719331bcf3",
    "ExecuteTime": {
     "end_time": "2025-05-08T19:13:15.250603Z",
     "start_time": "2025-05-08T19:13:15.223913Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "df_caggle"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "DrHmmib38aL6",
    "outputId": "08a268f9-7575-48a7-9ab7-3a5154bc0570",
    "ExecuteTime": {
     "end_time": "2025-05-08T19:13:19.841752Z",
     "start_time": "2025-05-08T19:13:19.830821Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       ID  Age  Experience  Income  ZIP Code  Family  CCAvg  Education  \\\n",
       "0       1   25           1      49     91107       4   1.60          1   \n",
       "1       2   45          19      34     90089       3   1.50          1   \n",
       "2       3   39          15      11     94720       1   1.00          1   \n",
       "3       4   35           9     100     94112       1   2.70          2   \n",
       "4       5   35           8      45     91330       4   1.00          2   \n",
       "..    ...  ...         ...     ...       ...     ...    ...        ...   \n",
       "995   996   28           3      45     94305       2   1.60          3   \n",
       "996   997   33           6      49     92037       2   1.67          2   \n",
       "997   998   46          20      69     92780       3   2.10          1   \n",
       "998   999   52          27      94     93106       1   2.80          2   \n",
       "999  1000   60          35      18     92120       1   1.50          2   \n",
       "\n",
       "     Mortgage  Personal Loan  Securities Account  CD Account  Online  \\\n",
       "0           0              0                   1           0       0   \n",
       "1           0              0                   1           0       0   \n",
       "2           0              0                   0           0       0   \n",
       "3           0              0                   0           0       0   \n",
       "4           0              0                   0           0       0   \n",
       "..        ...            ...                 ...         ...     ...   \n",
       "995         0              0                   0           0       1   \n",
       "996       214              0                   0           0       0   \n",
       "997         0              0                   0           0       0   \n",
       "998       333              0                   0           0       1   \n",
       "999         0              0                   0           0       1   \n",
       "\n",
       "     CreditCard  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             1  \n",
       "..          ...  \n",
       "995           1  \n",
       "996           0  \n",
       "997           0  \n",
       "998           0  \n",
       "999           1  \n",
       "\n",
       "[1000 rows x 14 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Income</th>\n",
       "      <th>ZIP Code</th>\n",
       "      <th>Family</th>\n",
       "      <th>CCAvg</th>\n",
       "      <th>Education</th>\n",
       "      <th>Mortgage</th>\n",
       "      <th>Personal Loan</th>\n",
       "      <th>Securities Account</th>\n",
       "      <th>CD Account</th>\n",
       "      <th>Online</th>\n",
       "      <th>CreditCard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>91107</td>\n",
       "      <td>4</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>34</td>\n",
       "      <td>90089</td>\n",
       "      <td>3</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>94720</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>94112</td>\n",
       "      <td>1</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "      <td>91330</td>\n",
       "      <td>4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>996</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>94305</td>\n",
       "      <td>2</td>\n",
       "      <td>1.60</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>997</td>\n",
       "      <td>33</td>\n",
       "      <td>6</td>\n",
       "      <td>49</td>\n",
       "      <td>92037</td>\n",
       "      <td>2</td>\n",
       "      <td>1.67</td>\n",
       "      <td>2</td>\n",
       "      <td>214</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>998</td>\n",
       "      <td>46</td>\n",
       "      <td>20</td>\n",
       "      <td>69</td>\n",
       "      <td>92780</td>\n",
       "      <td>3</td>\n",
       "      <td>2.10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>999</td>\n",
       "      <td>52</td>\n",
       "      <td>27</td>\n",
       "      <td>94</td>\n",
       "      <td>93106</td>\n",
       "      <td>1</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2</td>\n",
       "      <td>333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1000</td>\n",
       "      <td>60</td>\n",
       "      <td>35</td>\n",
       "      <td>18</td>\n",
       "      <td>92120</td>\n",
       "      <td>1</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 14 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "df_generated"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 756
    },
    "id": "51x11Un98ju4",
    "outputId": "19038b03-5f2f-441d-fd0a-d775476108e5",
    "ExecuteTime": {
     "end_time": "2025-05-08T19:13:20.033517Z",
     "start_time": "2025-05-08T19:13:20.022416Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     gender  birth_date  age                   city  postal_code  \\\n",
       "0    Female  1991-08-03   34              Bryanside        45471   \n",
       "1      Male  2001-07-10   24               Seanfurt        21310   \n",
       "2    Female  1969-09-28   56               Davebury          879   \n",
       "3      Male  1954-04-08   71               Chadland        22475   \n",
       "4      Male  1948-01-31   77             New Hannah        63590   \n",
       "..      ...         ...  ...                    ...          ...   \n",
       "995    Male  1943-09-09   82            East Evelyn        63863   \n",
       "996  Female  1957-01-09   68  Port Christopherville        14460   \n",
       "997  Female  1947-06-11   78          South Tiffany        32757   \n",
       "998  Female  1957-01-25   68              Erichaven        23654   \n",
       "999    Male  1999-02-22   26               Alextown        40106   \n",
       "\n",
       "                       country loan_purpose   education_level  \\\n",
       "0          Trinidad and Tobago          Car          Master's   \n",
       "1         United Arab Emirates     Business  Associate Degree   \n",
       "2          Trinidad and Tobago          Car        Bachelor's   \n",
       "3                      Belarus     Business               PhD   \n",
       "4          Trinidad and Tobago     Personal               PhD   \n",
       "..                         ...          ...               ...   \n",
       "995       United Arab Emirates    Education               PhD   \n",
       "996             Western Sahara    Education               PhD   \n",
       "997             Western Sahara    Education               PhD   \n",
       "998       United Arab Emirates     Business          Master's   \n",
       "999  Saint Pierre and Miquelon    Education          Master's   \n",
       "\n",
       "    employment_status  years_of_experience  ...      housing_status  \\\n",
       "0            Employed                    1  ...  Living with Family   \n",
       "1          Unemployed                    0  ...            Mortgage   \n",
       "2             Student                   35  ...                Rent   \n",
       "3             Student                   28  ...  Living with Family   \n",
       "4          Unemployed                   51  ...                Rent   \n",
       "..                ...                  ...  ...                 ...   \n",
       "995        Unemployed                   21  ...                Rent   \n",
       "996     Self-employed                    2  ...                Rent   \n",
       "997          Employed                   13  ...            Mortgage   \n",
       "998     Self-employed                   18  ...            Mortgage   \n",
       "999        Unemployed                    8  ...                Rent   \n",
       "\n",
       "           industry       disease chronic_condition   insurance_status  \\\n",
       "0    Transportation          None       Acid Reflux  Partially Insured   \n",
       "1       Hospitality          None              None          Uninsured   \n",
       "2           Finance      Diabetes  Seasonal Allergy  Partially Insured   \n",
       "3       Hospitality  Hypertension  Seasonal Allergy  Partially Insured   \n",
       "4                IT  Hypertension  Seasonal Allergy          Uninsured   \n",
       "..              ...           ...               ...                ...   \n",
       "995         Finance    Depression  Seasonal Allergy  Partially Insured   \n",
       "996    Construction    Depression         Arthritis          Uninsured   \n",
       "997    Construction      Diabetes              None            Insured   \n",
       "998      Government          None         Arthritis            Insured   \n",
       "999       Education  Hypertension  Seasonal Allergy          Uninsured   \n",
       "\n",
       "    monthly_medical_expenses_usd monthly_income_usd monthly_expenses_usd  \\\n",
       "0                         278.55            2114.35              1386.25   \n",
       "1                         868.34            1310.32              1113.01   \n",
       "2                        1740.94           20120.36              8986.48   \n",
       "3                         406.40           15725.94             14100.05   \n",
       "4                         138.17           31099.11             21390.76   \n",
       "..                           ...                ...                  ...   \n",
       "995                        95.55           14230.14             10762.25   \n",
       "996                       785.21            3286.58              2910.79   \n",
       "997                      1655.28            8994.30              7238.36   \n",
       "998                       972.61           10305.67              9175.70   \n",
       "999                      1127.41            5007.41              4295.81   \n",
       "\n",
       "     loan_amount_usd  credit_score  \n",
       "0           10231.34           666  \n",
       "1            4481.19           783  \n",
       "2           61557.95           641  \n",
       "3            8575.19           598  \n",
       "4           29449.51           491  \n",
       "..               ...           ...  \n",
       "995         61521.07           347  \n",
       "996           385.10           742  \n",
       "997          5005.53           537  \n",
       "998         41670.16           300  \n",
       "999          5351.98           454  \n",
       "\n",
       "[1000 rows x 23 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>birth_date</th>\n",
       "      <th>age</th>\n",
       "      <th>city</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>country</th>\n",
       "      <th>loan_purpose</th>\n",
       "      <th>education_level</th>\n",
       "      <th>employment_status</th>\n",
       "      <th>years_of_experience</th>\n",
       "      <th>...</th>\n",
       "      <th>housing_status</th>\n",
       "      <th>industry</th>\n",
       "      <th>disease</th>\n",
       "      <th>chronic_condition</th>\n",
       "      <th>insurance_status</th>\n",
       "      <th>monthly_medical_expenses_usd</th>\n",
       "      <th>monthly_income_usd</th>\n",
       "      <th>monthly_expenses_usd</th>\n",
       "      <th>loan_amount_usd</th>\n",
       "      <th>credit_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Female</td>\n",
       "      <td>1991-08-03</td>\n",
       "      <td>34</td>\n",
       "      <td>Bryanside</td>\n",
       "      <td>45471</td>\n",
       "      <td>Trinidad and Tobago</td>\n",
       "      <td>Car</td>\n",
       "      <td>Master's</td>\n",
       "      <td>Employed</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>Living with Family</td>\n",
       "      <td>Transportation</td>\n",
       "      <td>None</td>\n",
       "      <td>Acid Reflux</td>\n",
       "      <td>Partially Insured</td>\n",
       "      <td>278.55</td>\n",
       "      <td>2114.35</td>\n",
       "      <td>1386.25</td>\n",
       "      <td>10231.34</td>\n",
       "      <td>666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Male</td>\n",
       "      <td>2001-07-10</td>\n",
       "      <td>24</td>\n",
       "      <td>Seanfurt</td>\n",
       "      <td>21310</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>Business</td>\n",
       "      <td>Associate Degree</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Mortgage</td>\n",
       "      <td>Hospitality</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Uninsured</td>\n",
       "      <td>868.34</td>\n",
       "      <td>1310.32</td>\n",
       "      <td>1113.01</td>\n",
       "      <td>4481.19</td>\n",
       "      <td>783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Female</td>\n",
       "      <td>1969-09-28</td>\n",
       "      <td>56</td>\n",
       "      <td>Davebury</td>\n",
       "      <td>879</td>\n",
       "      <td>Trinidad and Tobago</td>\n",
       "      <td>Car</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Student</td>\n",
       "      <td>35</td>\n",
       "      <td>...</td>\n",
       "      <td>Rent</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Diabetes</td>\n",
       "      <td>Seasonal Allergy</td>\n",
       "      <td>Partially Insured</td>\n",
       "      <td>1740.94</td>\n",
       "      <td>20120.36</td>\n",
       "      <td>8986.48</td>\n",
       "      <td>61557.95</td>\n",
       "      <td>641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Male</td>\n",
       "      <td>1954-04-08</td>\n",
       "      <td>71</td>\n",
       "      <td>Chadland</td>\n",
       "      <td>22475</td>\n",
       "      <td>Belarus</td>\n",
       "      <td>Business</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Student</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>Living with Family</td>\n",
       "      <td>Hospitality</td>\n",
       "      <td>Hypertension</td>\n",
       "      <td>Seasonal Allergy</td>\n",
       "      <td>Partially Insured</td>\n",
       "      <td>406.40</td>\n",
       "      <td>15725.94</td>\n",
       "      <td>14100.05</td>\n",
       "      <td>8575.19</td>\n",
       "      <td>598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>1948-01-31</td>\n",
       "      <td>77</td>\n",
       "      <td>New Hannah</td>\n",
       "      <td>63590</td>\n",
       "      <td>Trinidad and Tobago</td>\n",
       "      <td>Personal</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>51</td>\n",
       "      <td>...</td>\n",
       "      <td>Rent</td>\n",
       "      <td>IT</td>\n",
       "      <td>Hypertension</td>\n",
       "      <td>Seasonal Allergy</td>\n",
       "      <td>Uninsured</td>\n",
       "      <td>138.17</td>\n",
       "      <td>31099.11</td>\n",
       "      <td>21390.76</td>\n",
       "      <td>29449.51</td>\n",
       "      <td>491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Male</td>\n",
       "      <td>1943-09-09</td>\n",
       "      <td>82</td>\n",
       "      <td>East Evelyn</td>\n",
       "      <td>63863</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>Education</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>Rent</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Depression</td>\n",
       "      <td>Seasonal Allergy</td>\n",
       "      <td>Partially Insured</td>\n",
       "      <td>95.55</td>\n",
       "      <td>14230.14</td>\n",
       "      <td>10762.25</td>\n",
       "      <td>61521.07</td>\n",
       "      <td>347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Female</td>\n",
       "      <td>1957-01-09</td>\n",
       "      <td>68</td>\n",
       "      <td>Port Christopherville</td>\n",
       "      <td>14460</td>\n",
       "      <td>Western Sahara</td>\n",
       "      <td>Education</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Self-employed</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>Rent</td>\n",
       "      <td>Construction</td>\n",
       "      <td>Depression</td>\n",
       "      <td>Arthritis</td>\n",
       "      <td>Uninsured</td>\n",
       "      <td>785.21</td>\n",
       "      <td>3286.58</td>\n",
       "      <td>2910.79</td>\n",
       "      <td>385.10</td>\n",
       "      <td>742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Female</td>\n",
       "      <td>1947-06-11</td>\n",
       "      <td>78</td>\n",
       "      <td>South Tiffany</td>\n",
       "      <td>32757</td>\n",
       "      <td>Western Sahara</td>\n",
       "      <td>Education</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Employed</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>Mortgage</td>\n",
       "      <td>Construction</td>\n",
       "      <td>Diabetes</td>\n",
       "      <td>None</td>\n",
       "      <td>Insured</td>\n",
       "      <td>1655.28</td>\n",
       "      <td>8994.30</td>\n",
       "      <td>7238.36</td>\n",
       "      <td>5005.53</td>\n",
       "      <td>537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Female</td>\n",
       "      <td>1957-01-25</td>\n",
       "      <td>68</td>\n",
       "      <td>Erichaven</td>\n",
       "      <td>23654</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>Business</td>\n",
       "      <td>Master's</td>\n",
       "      <td>Self-employed</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>Mortgage</td>\n",
       "      <td>Government</td>\n",
       "      <td>None</td>\n",
       "      <td>Arthritis</td>\n",
       "      <td>Insured</td>\n",
       "      <td>972.61</td>\n",
       "      <td>10305.67</td>\n",
       "      <td>9175.70</td>\n",
       "      <td>41670.16</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Male</td>\n",
       "      <td>1999-02-22</td>\n",
       "      <td>26</td>\n",
       "      <td>Alextown</td>\n",
       "      <td>40106</td>\n",
       "      <td>Saint Pierre and Miquelon</td>\n",
       "      <td>Education</td>\n",
       "      <td>Master's</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>Rent</td>\n",
       "      <td>Education</td>\n",
       "      <td>Hypertension</td>\n",
       "      <td>Seasonal Allergy</td>\n",
       "      <td>Uninsured</td>\n",
       "      <td>1127.41</td>\n",
       "      <td>5007.41</td>\n",
       "      <td>4295.81</td>\n",
       "      <td>5351.98</td>\n",
       "      <td>454</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 23 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": [
    "df_short = df_short.to_numpy()\n",
    "df_caggle = df_caggle.to_numpy()\n",
    "df_generated = df_generated.to_numpy()"
   ],
   "metadata": {
    "id": "Db98HM6N8oXj",
    "ExecuteTime": {
     "end_time": "2025-05-08T19:13:22.539716Z",
     "start_time": "2025-05-08T19:13:22.535349Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "Задать параметры датасетов:\n",
    "\n",
    "* quasi_identifiers_columns_ids - номера столбцов с квазиидентификаторами\n",
    "* quasi_identifiers_columns_types - типы столбцов с квазиидентификаторами (вещественные, порядковые или номинальные)\n",
    "* sensetive_columns_ids - номера столбцов с чувствительными значениями\n",
    "* sensetive_columns_types - типы столбцов с чувствительными значениями (вещественные, порядковые или номинальные)\n"
   ],
   "metadata": {
    "id": "Re2A9XCIcq4p"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KwZjWymz0gSA",
    "ExecuteTime": {
     "end_time": "2025-05-08T19:13:23.069776Z",
     "start_time": "2025-05-08T19:13:23.065432Z"
    }
   },
   "source": [
    "identifiers_columns_ids_caggle = [0]\n",
    "identifiers_columns_types_caggle = ['real']\n",
    "quasi_identifiers_columns_ids_caggle = [0,1,3,4,6,]\n",
    "quasi_identifiers_columns_types_caggle = ['real','real','unordered','real','real']\n",
    "sensetive_columns_ids_caggle = [2,5,7,8,9,10,11,12]\n",
    "sensetive_columns_types_caggle = ['real']*len(sensetive_columns_ids_caggle)\n",
    "\n",
    "quasi_identifiers_columns_ids_generated = [i for i in range(15)]\n",
    "quasi_identifiers_columns_types_generated = ['unordered','ordered','real','unordered','unordered','unordered','unordered','unordered','unordered','real','real','real','unordered','unordered','unordered']\n",
    "sensetive_columns_ids_generated = [i for i in range(15, 23)]\n",
    "sensetive_columns_types_generated = ['unordered','unordered','unordered','real','real','real','real','real']"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Эксперименты с алгоритмами обезличивания квазиидентификаторов для общих задач"
   ],
   "metadata": {
    "id": "n6Zmn4ZT0jso"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Задать параметры экспериментов:\n",
    "\n",
    "* наборы значений (k, l, t) для метрик k-анонимности, l-разнообразия и t-близости (3 массива одинаковой длины, из необходимых массивов выбираются значения с одинаковыми индексами)\n",
    "* значения scale для рандомизации\n",
    "* значение параметра генератора случайных чисел"
   ],
   "metadata": {
    "id": "9h2D0D-QhNAp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "k_s = [2, 10, 100]\n",
    "l_s = [2, 5, 20]\n",
    "t_s = [0.1, 0.5, 1]\n",
    "scales = [0.05, 0.25, 1]\n",
    "seed = 8"
   ],
   "metadata": {
    "id": "_p82QNjshTnJ",
    "ExecuteTime": {
     "end_time": "2025-05-08T19:13:24.609170Z",
     "start_time": "2025-05-08T19:13:24.606141Z"
    }
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "Все алгоритмы, рассматриваемые в этом разделе, подавляют все идентификаторы, поэтому для простоты работы идентификаторы заранее убираются из датасетов"
   ],
   "metadata": {
    "id": "JzV_zN2r41A3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Алгоритм time optimal"
   ],
   "metadata": {
    "id": "edOmHlm3dvDJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Подавление"
   ],
   "metadata": {
    "id": "-akz26Aff4-A"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### K-анонимность"
   ],
   "metadata": {
    "id": "CJ_Uc65OfO6a"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Получение обезличенного датасета и расчёт метрик полезности и безопасности"
   ],
   "metadata": {
    "id": "GFKCPVk_hsC_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for k in k_s:\n",
    "    print(f\"Обезличивание с k = {k}\\n\\n\")\n",
    "    print(f\"Обезличивание сгенерированного датасета\\n\")\n",
    "    depersonalized_df, _ = SuppressionKAnonymityTimeOptimal(k).depersonalize(df_generated, identifiers_ids=[], quasi_identifiers_ids=quasi_identifiers_columns_ids_generated, sensitives_ids=sensetive_columns_ids_generated)\n",
    "    score_depersonalization_quality(df_generated, depersonalized_df,quasi_identifiers_columns_ids_generated,quasi_identifiers_columns_types_generated,sensetive_columns_ids_generated,sensetive_columns_types_generated,2)\n",
    "    print(f\"\\nОбезличивание датасета с caggle\\n\")\n",
    "    depersonalized_df, _ = SuppressionKAnonymityTimeOptimal(k).depersonalize(df_caggle, identifiers_ids=[], quasi_identifiers_ids=quasi_identifiers_columns_ids_caggle, sensitives_ids=sensetive_columns_ids_caggle)\n",
    "    score_depersonalization_quality(df_caggle, depersonalized_df, quasi_identifiers_columns_ids_caggle, quasi_identifiers_columns_types_caggle, sensetive_columns_ids_caggle, sensetive_columns_types_caggle, 0)\n",
    "    print(f\"\\nОбезличивание короткого датасета\\n\")\n",
    "    depersonalized_df, _ = SuppressionKAnonymityTimeOptimal(k).depersonalize(df_short, identifiers_ids=[], quasi_identifiers_ids=quasi_identifiers_columns_ids_caggle, sensitives_ids=sensetive_columns_ids_caggle)\n",
    "    score_depersonalization_quality(df_short, depersonalized_df, quasi_identifiers_columns_ids_caggle, quasi_identifiers_columns_types_caggle, sensetive_columns_ids_caggle, sensetive_columns_types_caggle, 0)\n",
    "    print(\"\\n\\n======================================================\\n\\n\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tdlDOM42eCIH",
    "outputId": "f20b98ae-d348-47a9-a9ac-c9ebf3ff2160",
    "ExecuteTime": {
     "end_time": "2025-05-08T19:17:35.837359Z",
     "start_time": "2025-05-08T19:16:13.363818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обезличивание с k = 2\n",
      "\n",
      "\n",
      "Обезличивание сгенерированного датасета\n",
      "\n",
      "Максимальное k: 2\n",
      "security boosting score: 0.145\n",
      "2.0\n",
      "0.5\n",
      "7.942\n",
      "8.965784284662082\n",
      "0.10034333188799394\n",
      "0.5294666666666666\n",
      "23.430120364032106\n",
      "\n",
      "Обезличивание датасета с caggle\n",
      "\n",
      "Максимальное k: 2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nGradientBoostingClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 8\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mОбезличивание датасета с caggle\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      7\u001B[0m depersonalized_df, _ \u001B[38;5;241m=\u001B[39m SuppressionKAnonymityTimeOptimal(k)\u001B[38;5;241m.\u001B[39mdepersonalize(df_caggle, identifiers_ids\u001B[38;5;241m=\u001B[39m[], quasi_identifiers_ids\u001B[38;5;241m=\u001B[39mquasi_identifiers_columns_ids_caggle, sensitives_ids\u001B[38;5;241m=\u001B[39msensetive_columns_ids_caggle)\n\u001B[1;32m----> 8\u001B[0m score_depersonalization_quality(df_caggle, depersonalized_df, quasi_identifiers_columns_ids_caggle, quasi_identifiers_columns_types_caggle, sensetive_columns_ids_caggle, sensetive_columns_types_caggle, \u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mОбезличивание короткого датасета\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     10\u001B[0m depersonalized_df, _ \u001B[38;5;241m=\u001B[39m SuppressionKAnonymityTimeOptimal(k)\u001B[38;5;241m.\u001B[39mdepersonalize(df_short, identifiers_ids\u001B[38;5;241m=\u001B[39m[], quasi_identifiers_ids\u001B[38;5;241m=\u001B[39mquasi_identifiers_columns_ids_caggle, sensitives_ids\u001B[38;5;241m=\u001B[39msensetive_columns_ids_caggle)\n",
      "Cell \u001B[1;32mIn[2], line 18\u001B[0m, in \u001B[0;36mscore_depersonalization_quality\u001B[1;34m(initial_df, depersonalized_df, quasi_identifiers_columns_ids, quasi_identifiers_columns_types, sensetive_columns_ids, sensetive_columns_types, predicted_column, generalization)\u001B[0m\n\u001B[0;32m     16\u001B[0m train_initial, test_initial \u001B[38;5;241m=\u001B[39m prepare_data(train_initial_before_prepare, test_initial_before_prepare, [generalization] \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mlen\u001B[39m(quasi_identifiers_columns_types) \u001B[38;5;241m+\u001B[39m [\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mlen\u001B[39m(sensetive_columns_types), quasi_identifiers_columns_types \u001B[38;5;241m+\u001B[39m sensetive_columns_types, normalize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m     17\u001B[0m train_depersonalized, test_depersonalized \u001B[38;5;241m=\u001B[39m prepare_data(train_depersonalized, test_depersonalized, [generalization] \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mlen\u001B[39m(quasi_identifiers_columns_types) \u001B[38;5;241m+\u001B[39m [\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mlen\u001B[39m(sensetive_columns_types), quasi_identifiers_columns_types \u001B[38;5;241m+\u001B[39m sensetive_columns_types, normalize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m---> 18\u001B[0m security_boosting_score \u001B[38;5;241m=\u001B[39m get_boosting_security_score(train_initial, train_depersonalized, test_initial, test_depersonalized)\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msecurity boosting score: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msecurity_boosting_score\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     21\u001B[0m \u001B[38;5;66;03m# utility\u001B[39;00m\n",
      "File \u001B[1;32mC:\\GitHub\\CP_2025\\cp2025\\utility\\boosting_security_score.py:17\u001B[0m, in \u001B[0;36mget_boosting_security_score\u001B[1;34m(train, train_depersonalized, test, test_depersonalized)\u001B[0m\n\u001B[0;32m     14\u001B[0m boosting_train \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mconcatenate((similar_train, different_train), axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m     16\u001B[0m clf \u001B[38;5;241m=\u001B[39m GradientBoostingClassifier(random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, n_estimators\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m)\n\u001B[1;32m---> 17\u001B[0m clf\u001B[38;5;241m.\u001B[39mfit(boosting_train, new_y)\n\u001B[0;32m     19\u001B[0m correct_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(test\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]):\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1467\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1469\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1470\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1471\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1472\u001B[0m     )\n\u001B[0;32m   1473\u001B[0m ):\n\u001B[1;32m-> 1474\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fit_method(estimator, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:659\u001B[0m, in \u001B[0;36mBaseGradientBoosting.fit\u001B[1;34m(self, X, y, sample_weight, monitor)\u001B[0m\n\u001B[0;32m    653\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_clear_state()\n\u001B[0;32m    655\u001B[0m \u001B[38;5;66;03m# Check input\u001B[39;00m\n\u001B[0;32m    656\u001B[0m \u001B[38;5;66;03m# Since check_array converts both X and y to the same dtype, but the\u001B[39;00m\n\u001B[0;32m    657\u001B[0m \u001B[38;5;66;03m# trees use different types for X and y, checking them separately.\u001B[39;00m\n\u001B[1;32m--> 659\u001B[0m X, y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_data(\n\u001B[0;32m    660\u001B[0m     X, y, accept_sparse\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsr\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsc\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcoo\u001B[39m\u001B[38;5;124m\"\u001B[39m], dtype\u001B[38;5;241m=\u001B[39mDTYPE, multi_output\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    661\u001B[0m )\n\u001B[0;32m    662\u001B[0m sample_weight_is_none \u001B[38;5;241m=\u001B[39m sample_weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    663\u001B[0m sample_weight \u001B[38;5;241m=\u001B[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:650\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001B[0m\n\u001B[0;32m    648\u001B[0m         y \u001B[38;5;241m=\u001B[39m check_array(y, input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_y_params)\n\u001B[0;32m    649\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 650\u001B[0m         X, y \u001B[38;5;241m=\u001B[39m check_X_y(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_params)\n\u001B[0;32m    651\u001B[0m     out \u001B[38;5;241m=\u001B[39m X, y\n\u001B[0;32m    653\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m check_params\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mensure_2d\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m):\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1263\u001B[0m, in \u001B[0;36mcheck_X_y\u001B[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001B[0m\n\u001B[0;32m   1258\u001B[0m         estimator_name \u001B[38;5;241m=\u001B[39m _check_estimator_name(estimator)\n\u001B[0;32m   1259\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1260\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mestimator_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m requires y to be passed, but the target y is None\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1261\u001B[0m     )\n\u001B[1;32m-> 1263\u001B[0m X \u001B[38;5;241m=\u001B[39m check_array(\n\u001B[0;32m   1264\u001B[0m     X,\n\u001B[0;32m   1265\u001B[0m     accept_sparse\u001B[38;5;241m=\u001B[39maccept_sparse,\n\u001B[0;32m   1266\u001B[0m     accept_large_sparse\u001B[38;5;241m=\u001B[39maccept_large_sparse,\n\u001B[0;32m   1267\u001B[0m     dtype\u001B[38;5;241m=\u001B[39mdtype,\n\u001B[0;32m   1268\u001B[0m     order\u001B[38;5;241m=\u001B[39morder,\n\u001B[0;32m   1269\u001B[0m     copy\u001B[38;5;241m=\u001B[39mcopy,\n\u001B[0;32m   1270\u001B[0m     force_all_finite\u001B[38;5;241m=\u001B[39mforce_all_finite,\n\u001B[0;32m   1271\u001B[0m     ensure_2d\u001B[38;5;241m=\u001B[39mensure_2d,\n\u001B[0;32m   1272\u001B[0m     allow_nd\u001B[38;5;241m=\u001B[39mallow_nd,\n\u001B[0;32m   1273\u001B[0m     ensure_min_samples\u001B[38;5;241m=\u001B[39mensure_min_samples,\n\u001B[0;32m   1274\u001B[0m     ensure_min_features\u001B[38;5;241m=\u001B[39mensure_min_features,\n\u001B[0;32m   1275\u001B[0m     estimator\u001B[38;5;241m=\u001B[39mestimator,\n\u001B[0;32m   1276\u001B[0m     input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1277\u001B[0m )\n\u001B[0;32m   1279\u001B[0m y \u001B[38;5;241m=\u001B[39m _check_y(y, multi_output\u001B[38;5;241m=\u001B[39mmulti_output, y_numeric\u001B[38;5;241m=\u001B[39my_numeric, estimator\u001B[38;5;241m=\u001B[39mestimator)\n\u001B[0;32m   1281\u001B[0m check_consistent_length(X, y)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1049\u001B[0m, in \u001B[0;36mcheck_array\u001B[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[0m\n\u001B[0;32m   1043\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1044\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound array with dim \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m expected <= 2.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1045\u001B[0m         \u001B[38;5;241m%\u001B[39m (array\u001B[38;5;241m.\u001B[39mndim, estimator_name)\n\u001B[0;32m   1046\u001B[0m     )\n\u001B[0;32m   1048\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m force_all_finite:\n\u001B[1;32m-> 1049\u001B[0m     _assert_all_finite(\n\u001B[0;32m   1050\u001B[0m         array,\n\u001B[0;32m   1051\u001B[0m         input_name\u001B[38;5;241m=\u001B[39minput_name,\n\u001B[0;32m   1052\u001B[0m         estimator_name\u001B[38;5;241m=\u001B[39mestimator_name,\n\u001B[0;32m   1053\u001B[0m         allow_nan\u001B[38;5;241m=\u001B[39mforce_all_finite \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow-nan\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1054\u001B[0m     )\n\u001B[0;32m   1056\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m copy:\n\u001B[0;32m   1057\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_numpy_namespace(xp):\n\u001B[0;32m   1058\u001B[0m         \u001B[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:126\u001B[0m, in \u001B[0;36m_assert_all_finite\u001B[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001B[0m\n\u001B[0;32m    123\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m first_pass_isfinite:\n\u001B[0;32m    124\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m--> 126\u001B[0m _assert_all_finite_element_wise(\n\u001B[0;32m    127\u001B[0m     X,\n\u001B[0;32m    128\u001B[0m     xp\u001B[38;5;241m=\u001B[39mxp,\n\u001B[0;32m    129\u001B[0m     allow_nan\u001B[38;5;241m=\u001B[39mallow_nan,\n\u001B[0;32m    130\u001B[0m     msg_dtype\u001B[38;5;241m=\u001B[39mmsg_dtype,\n\u001B[0;32m    131\u001B[0m     estimator_name\u001B[38;5;241m=\u001B[39mestimator_name,\n\u001B[0;32m    132\u001B[0m     input_name\u001B[38;5;241m=\u001B[39minput_name,\n\u001B[0;32m    133\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:175\u001B[0m, in \u001B[0;36m_assert_all_finite_element_wise\u001B[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001B[0m\n\u001B[0;32m    158\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m estimator_name \u001B[38;5;129;01mand\u001B[39;00m input_name \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m has_nan_error:\n\u001B[0;32m    159\u001B[0m     \u001B[38;5;66;03m# Improve the error message on how to handle missing values in\u001B[39;00m\n\u001B[0;32m    160\u001B[0m     \u001B[38;5;66;03m# scikit-learn.\u001B[39;00m\n\u001B[0;32m    161\u001B[0m     msg_err \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    162\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mestimator_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m does not accept missing values\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    163\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    173\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m#estimators-that-handle-nan-values\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    174\u001B[0m     )\n\u001B[1;32m--> 175\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg_err)\n",
      "\u001B[1;31mValueError\u001B[0m: Input X contains NaN.\nGradientBoostingClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### L-разнообразие"
   ],
   "metadata": {
    "id": "e_TF0fsYfSsv"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "0HmMotD-fYVt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Обобщение"
   ],
   "metadata": {
    "id": "bA5u1h8Je8YU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### K-анонимность"
   ],
   "metadata": {
    "id": "2rsvOEMxfcNn"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "xHb6xSVufDdW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### L-разнообразие"
   ],
   "metadata": {
    "id": "_tcVgeEUfgIY"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "5j8aFboffhTC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Агрегация"
   ],
   "metadata": {
    "id": "tkIlKcmvfDwH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### K-анонимность"
   ],
   "metadata": {
    "id": "Nl4EwukBfjdy"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "G3MYKllwfMXM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### L-разнообразие"
   ],
   "metadata": {
    "id": "mxHj_ZZ7fiNa"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "TwK0sW1bfi9M"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Алгоритм group join"
   ],
   "metadata": {
    "id": "UILBqtaJeEAL"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Подавление"
   ],
   "metadata": {
    "id": "nwFlh9S_gGkI"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### K-анонимность"
   ],
   "metadata": {
    "id": "at_PneiwgQ5G"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "UTGuITXjgc_C"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### L-разнообразие"
   ],
   "metadata": {
    "id": "dSyniNFRgVdt"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "fVYcH73Agdbh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### T-близость"
   ],
   "metadata": {
    "id": "iPtm3yTTgY7n"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "5bXoUo0rgd0x"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Обобщение"
   ],
   "metadata": {
    "id": "nlsUA224gJrl"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### K-анонимность"
   ],
   "metadata": {
    "id": "aZns4ESCgRtT"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "2fvB4sVMgeX9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### L-разнообразие"
   ],
   "metadata": {
    "id": "fTHhvVZ-gWYg"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "912pZcmpgetB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### T-близость"
   ],
   "metadata": {
    "id": "jH1DgY5ngbmW"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "if0VIGrCgfBJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Агрегация"
   ],
   "metadata": {
    "id": "n3e0afumgLkn"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### K-анонимность"
   ],
   "metadata": {
    "id": "aBgYnhrtgSYG"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "MzWzohb1gfph"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### L-разнообразие"
   ],
   "metadata": {
    "id": "bq0QWpZGgW3m"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "ZPR15mzegf8Q"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### T-близость"
   ],
   "metadata": {
    "id": "ylv7V0vAgcO1"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "JrbHAjX_eLmn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Алгоритм Datafly"
   ],
   "metadata": {
    "id": "0SBacr4sePIz"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "NS2iqps0eZ6H"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Алгоритм разбиения столбцов на группы с равным минимальными размером"
   ],
   "metadata": {
    "id": "VQ45ewbNeaf1"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "XiW_fIaHetN_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Рандомизация"
   ],
   "metadata": {
    "id": "a6vSiSaEetpf"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "lH8V6AiGexUf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Алгоритм перебора с подавлением"
   ],
   "metadata": {
    "id": "x6a7a2f7DhtP"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### K-анонимность"
   ],
   "metadata": {
    "id": "po1Y9JIIDpG9"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "PVIiJkFNDrwJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### L-разнообразие"
   ],
   "metadata": {
    "id": "KEl3u6rBDsaM"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "-RThjPwGDuFI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### T-близость"
   ],
   "metadata": {
    "id": "nf_T7MU8DuZh"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "I3gM8__bDvF1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Эксперименты с алгоритмами для специальных задач"
   ],
   "metadata": {
    "id": "LzV1kBFciJKl"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "g4Foc_XwiQII"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
